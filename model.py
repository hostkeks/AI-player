import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


def load_gemma_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Gemma-2-2b"""
    model_name = "./gemma"

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16
    )

    # –î–æ–±–∞–≤–ª—è–µ–º padding token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    return model, tokenizer


def clean_response(response, prompt):
    """–û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —á–∞—Å—Ç–µ–π"""
    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
    if prompt in response:
        response = response.replace(prompt, "")

    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Ç–µ–≥–∏ –∏ –∫–æ–¥
    response = response.split("<end_of_turn>")[0]
    response = response.split("```")[0]
    response = response.strip()

    return response


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —á–∞—Ç–∞"""
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Gemma-2-2b...")
    model, tokenizer = load_gemma_model()
    print("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    print("\nü§ñ Gemma Chat –≥–æ—Ç–æ–≤ –∫ –æ–±—â–µ–Ω–∏—é!")
    print("–ù–∞–ø–∏—à–∏—Ç–µ '–≤—ã—Ö–æ–¥' —á—Ç–æ–±—ã –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä\n")

    while True:
        user_input = input("–í—ã: ")

        if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit', 'bye']:
            print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! üëã")
            break

        if not user_input.strip():
            continue

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è —á–∞—Ç–∞
        prompt = f"<start_of_turn>user\n{user_input}<end_of_turn>\n<start_of_turn>model\n"

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
        inputs = inputs.to(model.device)

        print("Gemma –¥—É–º–∞–µ—Ç...")
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    top_k=50,
                    repetition_penalty=1.1,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )

            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)

            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            response = clean_response(full_response, prompt)

            print(f"Gemma: {response}\n")

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()